{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <center>ING2 Ingénierie Mathématiques</center>\n",
    "### <center>Décembre 2025</center>\n",
    "## <center>Devoir Maison: Réseaux Convolutifs</center>\n",
    "\n",
    "\n",
    "<center><font color='red'><b>date de début: 19 decembre 2025</b></font></center>\n",
    "\n",
    "<center><font color='red'><b>date de remise: 03 Janvier 2025 </b></font></center>\n",
    "\n",
    "\n",
    "#### <center>Total: 30pts</center>\n",
    "\n",
    "Références: \n",
    "\n",
    " - [Ian Goodfellow and Yoshua Bengio and Aaron Courville, Deep Learning](https://www.deeplearningbook.org/)\n",
    " - [Saharon Rosset, Ji Zhu and Trevor Hastie, Margin Maximizing Loss Functions](https://web.stanford.edu/~hastie/Papers/margmax1.pdf)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question I: (15pts) réseaux convolutifs et CIFAR \n",
    "\n",
    "Dans la premiere question, on va utiliser l'interface de programmation [Keras](https://keras.io/) pour assembler et entrainer un reseau convolutif de façon à distinguer deux type d'images issues du jeu de données CIFAR-10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rendez vous a l'adresse  https://www.cs.toronto.edu/~kriz/cifar.html et télécharger le jeu de données CIFAR-10. Sélectionner deux classes parmi les 10 classes du jeux de données. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question I.1. (10pts) \n",
    "\n",
    "Dans cette première partie, on va entraîner un réseau convolutif de façon à différencier les deux classes. \n",
    "\n",
    "- On va utiliser l'architecture __Séquentielle__ de Keras à partir de laquelle vous allez assembler le réseau convolutionel. L'Instantiation du modèle séquentiel peut se faire via les lignes suivantes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow\n",
    "#%pip install keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.a. Convolutions. \n",
    "\n",
    "- Dans la construction du réseau convolutif, on va utiliser des couches de convolution. Les couches de convolution peuvent être ajoutées via la ligne suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(\n",
    "    filters=32, \n",
    "    kernel_size=(3, 3), \n",
    "    padding='same',\n",
    "    input_shape=(32, 32, 3),\n",
    "    activation='relu'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la première couche et "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(\n",
    "    filters=64, \n",
    "    kernel_size=(3, 3), \n",
    "    activation='relu'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pour les couches suivantes. Le paramètre 'filters' indique le nombre de filtres apparaissant dans la couche de convolution. Le paramètre 'filter_size' indique la dimension de chaque filtre et le paramètre 'activation' correspond aux choix de la fonction d'activation appliqué au résultat de la convolution. I.e.  \n",
    "\n",
    "$x_{\\text{out}} = \\sigma(\\text{filter}*\\text{input})$. \n",
    "\n",
    "Finalement le paramètre 'input_shape' indique la taille de l'entrée du réseau. On notera que seule la couche d'entrée nécéssite de spécifier la dimension des données. Les couches suivantes calculent automatiquement la taille de leurs entrées sur base des sorties des couches précédentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.b Couches de pooling\n",
    "\n",
    "En sus des couches de convolution, les réseaux de neurones convolutifs (CNN) contiennent aussi des __Couches de pooling__. \n",
    "l'ajout des couches de pooling peut se faire via les lignes suivantes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2, 2),strides=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les _couches de pooling_ sont généralement instantiées au moyen de deux paramètres : le paramètre 'pool size' et le paramètre 'stride'. la valeur par défaut du parametre 'pool_size' est (2,2) et le stride est généralement fixé a 'None' (ce qui vetu dire qu'il divise l'image en regions distinctes comme indique dans la figure ci dessous). On pourra tester différentes valeurs de ces paramètres. L'operateur __MaxPool operator__ prend en argument un masque de taille 'pool_size' qui est translate sur l'image d'une distance égale à un nombre de pixel égal au paramètre du stride (en x et y, il y a deux paramètres de translation). Pour chaque position du masque, la sortie reprend uniquement le maximum des pixels apparaissant sur le masque (L'idée est illustrée ci-dessous). Une manière possible de comprendre l'effet de l'opérateur de pooling est que si le filtre détecte une arête dans une sous-région de l'image (retournant donc une valeur plus importante), bien que l'opérateur MaxPooling réduise le nombre de paramètres, il propagera l'information.\n",
    "\n",
    "Il est généralement admis que l'ajout de couches de type 'Maxpooling' améliore le fonctionnement des réseaux de neurones.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"maxPool.png\" style=\"width:500px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien que ce soit à vous de déterminer la structure optimale du réseau, un bon début consiste en l'addition de quelques (au maximum 4) combinaisons de la forme (convolution, convolution, pooling) avec un nombre de plus en plus important de neurones (on pourra par exemple utiliser des puissances de 2: 16, 32, 128,...). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.c. Couches d'appltissement et couches complètement connectées\n",
    "\n",
    "Une fois les couches convolutives associées au réseau, la sortie est généralement transformée en un vecteur via une couche d'applatissement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On concluera la structure en ajoutant 2,3 couches complètement connectées via la ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.d. Conclusion\n",
    "\n",
    "Etant donné qu'il y deux classes d'images possibles. on veillera à __terminer le réseau avec une couche dense de 2 neurones__.\n",
    "On veillera a ce que chacun de ces neurones renvoie un nombre entre 0 et 1 indiquant la probabilité qu'une image donnée appartienne à chacune des deux classes. On a donc $p_1 + p_2 = 1$ (avec l'espoir qu'une des probabilité soit plus grande que les autres). Pour toutes ces raisons, il peut être judicieux de choisir comme __fonction d'activation de sortie__ la fonction sigmoide ou une fonction softmax. \n",
    "\n",
    "Construisez votre modèle ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "\n",
    "#model = Sequential()\n",
    "# construct the model using convolutional layers, dense fully connected layers and pooling layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2. Optimisation (3pts).\n",
    "\n",
    "Une fois définie l'architecture du réseau, on commencera par scinder les données MNIST en un ensemble d'entrainement (90% des images) et un ensemble de test (10% restants). Pour entrainer le réseau en Keras, on aura besoin de deux étapes supplémentaires. La première étape consistera en l'instantiation de l'algorithme d'optimisation. L'interface keras fournit différents modèles parmi lesquels on retrouve les traditionnels __gradient sctochastique__ et __ADAM__. On veillera à choisir le taux d'apprentissage (une bonne idée consiste à prendre un taux d'apprentissage entre 1e-3 et 1e-2)\n",
    "\n",
    "Une fois instantié l'algorithme d'optimisation, on définira la fonction de coût (dans ce cas-ci on pourra prendre la fonction __d'entropie binaire croisee__, __keras.losses.BinaryCrossentropy__.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# set up the optimize here\n",
    "# Myoptimizer = SGD\n",
    "# Myoptimizer = Adam\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=SGD(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question I.3 (2pts). Optimization\n",
    "\n",
    "La dernière étape consiste en l'entrainement du réseau convolutif à l'aide des donnees d'entrainement. Comme pour les modèles scikit-learn, keras implémente l'entrainement des réseaux de neurones via la méthode 'fit'. \n",
    "\n",
    "L'entrainement du modèle se fait généralement en séparant les données d'entrainement en compartiments (minibatches) et en utilisant un compartiment différent a chaque itération de gradient. La procédure est repétée sur l'ensemble des données d'entrainement. Un balayage complet des données est appelé __epoch__. L'entrainement est ensuite répété sur un nombre fixé d'epochs. \n",
    "En Keras, le nombre d'epochs est enregistré par le paramètre 'epochs' de la méthode 'fit' et la taille des compartiments est stockée dans le paramètre 'batch_size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Chargement des données\n",
    "(X_full, t_full), (X_test_full, t_test_full) = cifar10.load_data()\n",
    "\n",
    "# Sélection de deux classes : 0 (avion) et 1 (auto)\n",
    "def filter_classes(X_data, t_data):\n",
    "    mask = (t_data == 0) | (t_data == 1)\n",
    "    mask = mask.flatten()\n",
    "    return X_data[mask], t_data[mask]\n",
    "\n",
    "X, t = filter_classes(X_full, t_full)\n",
    "X_test, t_test = filter_classes(X_test_full, t_test_full)\n",
    "\n",
    "# Normalisation (0 à 1)\n",
    "X = X.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# IMPORTANT : Selon votre code précédent, vous utilisiez (3, 32, 32)\n",
    "# Si vous gardez input_shape=(32, 32, 3), ne faites pas le transpose.\n",
    "# Ici, on suit l'énoncé I.1.a qui propose (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.5687 - loss: 0.6789 - val_accuracy: 0.7480 - val_loss: 0.6458\n",
      "Epoch 2/10\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - accuracy: 0.7405 - loss: 0.6299 - val_accuracy: 0.7400 - val_loss: 0.5889\n",
      "Epoch 3/10\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.7512 - loss: 0.5730 - val_accuracy: 0.7620 - val_loss: 0.5368\n",
      "Epoch 4/10\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.7762 - loss: 0.5259 - val_accuracy: 0.7730 - val_loss: 0.5001\n",
      "Epoch 5/10\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.7831 - loss: 0.4904 - val_accuracy: 0.7900 - val_loss: 0.4720\n",
      "Epoch 6/10\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - accuracy: 0.8000 - loss: 0.4608 - val_accuracy: 0.8000 - val_loss: 0.4529\n",
      "Epoch 7/10\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.8042 - loss: 0.4468 - val_accuracy: 0.8080 - val_loss: 0.4448\n",
      "Epoch 8/10\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.8040 - loss: 0.4420 - val_accuracy: 0.8200 - val_loss: 0.4221\n",
      "Epoch 9/10\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.8169 - loss: 0.4201 - val_accuracy: 0.8270 - val_loss: 0.4124\n",
      "Epoch 10/10\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step - accuracy: 0.8204 - loss: 0.4080 - val_accuracy: 0.8090 - val_loss: 0.4206\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    X, t, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs, \n",
    "    validation_split=0.1 # 90% train, 10% validation\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
